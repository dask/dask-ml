

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Dask GLM &mdash; dask-ml 0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="dask-ml 0.1 documentation" href="../index.html"/>
        <link rel="up" title="&lt;no title&gt;" href="../examples.html"/>
        <link rel="next" title="Dask and XGBoost" href="xgboost.html"/>
        <link rel="prev" title="Out-of-core Prediction" href="predict.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> dask-ml
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../single-machine.html">Single Machine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed.html">Distributed Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="hyperparameter-search.html">Hyper-parameter Search</a></li>
<li class="toctree-l1"><a class="reference internal" href="joblib-distributed.html">Distributed Joblib</a></li>
<li class="toctree-l1"><a class="reference internal" href="predict.html">Out-of-core Prediction</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Dask GLM</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Pipelines">Pipelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Grid-Search">Grid Search</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="xgboost.html">Dask and XGBoost</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorflow.html">Dask and Tensorflow</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">dask-ml</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../examples.html">&lt;no title&gt;</a> &raquo;</li>
        
      <li>Dask GLM</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/examples/dask-glm.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="Dask-GLM">
<h1>Dask GLM<a class="headerlink" href="#Dask-GLM" title="Permalink to this headline">¶</a></h1>
<p><code class="docutils literal"><span class="pre">`dask-glm</span></code> &lt;github.com/dask/dask-glm&gt;`__ is a library for fitting
generalized linear models on large datasets. The heart of the project is
the set of optimization routines that work on either NumPy or dask
arrays. See
<a class="reference external" href="https://mrocklin.github.com/blog/work/2017/03/22/dask-glm-1">these</a>
<a class="reference external" href="http://matthewrocklin.com/blog/work/2017/04/19/dask-glm-2">two</a>
blogposts describing how dask-glm works internally.</p>
<p>This notebook is shows an example of the higher-level scikit-learn style
API built on top of these optimization routines.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">s3fs</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">dask.array</span> <span class="k">as</span> <span class="nn">da</span>
<span class="kn">import</span> <span class="nn">dask.dataframe</span> <span class="k">as</span> <span class="nn">dd</span>
<span class="kn">from</span> <span class="nn">distributed</span> <span class="k">import</span> <span class="n">Client</span>

<span class="kn">from</span> <span class="nn">dask</span> <span class="k">import</span> <span class="n">persist</span><span class="p">,</span> <span class="n">compute</span>
<span class="kn">from</span> <span class="nn">dask_glm.estimators</span> <span class="k">import</span> <span class="n">LogisticRegression</span>
</pre></div>
</div>
</div>
<p>We’ll setup a
<code class="docutils literal"><span class="pre">`distributed.Client</span></code> &lt;<a class="reference external" href="http://distributed.readthedocs.io/en/latest/api.html#distributed.client.Client">http://distributed.readthedocs.io/en/latest/api.html#distributed.client.Client</a>&gt;`__
locally. In the real world you could connect to a cluster of
dask-workers.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>For demonstration, we’ll use the perennial NYC taxi cab dataset. Since
I’m just running things on my laptop, we’ll just grab the first month’s
worth of data.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">ddf</span> <span class="o">=</span> <span class="n">dd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;trip.csv&quot;</span><span class="p">)</span>
<span class="n">ddf</span> <span class="o">=</span> <span class="n">ddf</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="n">npartitions</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>I happen to know that some of the values in this dataset are suspect, so
let’s drop them. Scikit-learn doesn’t support filtering observations
inside a pipeline
(<a class="reference external" href="https://github.com/scikit-learn/scikit-learn/issues/3855">yet</a>), so
we’ll do this before anything else.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="c1"># these filter out less than 1% of the observations</span>
<span class="n">ddf</span> <span class="o">=</span> <span class="n">ddf</span><span class="p">[(</span><span class="n">ddf</span><span class="o">.</span><span class="n">trip_distance</span> <span class="o">&lt;</span> <span class="mi">20</span><span class="p">)</span> <span class="o">&amp;</span>
          <span class="p">(</span><span class="n">ddf</span><span class="o">.</span><span class="n">fare_amount</span> <span class="o">&lt;</span> <span class="mi">150</span><span class="p">)]</span>
<span class="n">ddf</span> <span class="o">=</span> <span class="n">ddf</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="n">npartitions</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now, we’ll split our DataFrame into a train and test set, and select our
feature matrix and target column (whether the passenger tipped).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span> <span class="o">=</span> <span class="n">ddf</span><span class="o">.</span><span class="n">random_split</span><span class="p">([</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;VendorID&#39;</span><span class="p">,</span> <span class="s1">&#39;passenger_count&#39;</span><span class="p">,</span> <span class="s1">&#39;trip_distance&#39;</span><span class="p">,</span> <span class="s1">&#39;payment_type&#39;</span><span class="p">,</span> <span class="s1">&#39;fare_amount&#39;</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">[</span><span class="n">columns</span><span class="p">],</span> <span class="n">df_train</span><span class="p">[</span><span class="s1">&#39;tip_amount&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="n">columns</span><span class="p">],</span> <span class="n">df_test</span><span class="p">[</span><span class="s1">&#39;tip_amount&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">persist</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">X_train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[7]:
</pre></div>
</div>
<div class="output_area docutils container">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>VendorID</th>
      <th>passenger_count</th>
      <th>trip_distance</th>
      <th>payment_type</th>
      <th>fare_amount</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>1</td>
      <td>1.8</td>
      <td>2</td>
      <td>9.5</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>0.5</td>
      <td>2</td>
      <td>3.5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>1</td>
      <td>3.0</td>
      <td>2</td>
      <td>15.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1</td>
      <td>1</td>
      <td>9.0</td>
      <td>1</td>
      <td>27.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1</td>
      <td>1</td>
      <td>2.2</td>
      <td>2</td>
      <td>14.0</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">y_train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[8]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>2    False
3    False
4    False
5     True
6    False
Name: tip_amount, dtype: bool
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;{len(X_train):,d} observations&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
10,155,301 observations
</pre></div></div>
</div>
<p>With our training data in hand, we fit our logistic regression. Nothing
here should be surprising to those familiar with <code class="docutils literal"><span class="pre">scikit-learn</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="o">%%</span><span class="k">time</span>
# this is a *dask-glm* LogisticRegresion, not scikit-learn
lm = LogisticRegression(fit_intercept=False)
lm.fit(X_train.values, y_train.values)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 1min 27s, sys: 13.3 s, total: 1min 40s
Wall time: 11min 25s
</pre></div></div>
</div>
<p>Again, following the lead of scikit-learn we can measure the performance
of the estimator on the training dataset using the <code class="docutils literal"><span class="pre">.score</span></code> method.
For LogisticRegression this is the mean accuracy score (what percent of
the predicted matched the actual).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="o">%%</span><span class="k">time</span>
lm.score(X_train.values, y_train.values).compute()
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 205 ms, sys: 25 ms, total: 230 ms
Wall time: 364 ms
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[11]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.88082578743850137
</pre></div>
</div>
</div>
<p>and on the test dataset:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="o">%%</span><span class="k">time</span>
lm.score(X_test.values, y_test.values).compute()
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 144 ms, sys: 21.8 ms, total: 166 ms
Wall time: 249 ms
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[12]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.88061000067744588
</pre></div>
</div>
</div>
<div class="section" id="Pipelines">
<h2>Pipelines<a class="headerlink" href="#Pipelines" title="Permalink to this headline">¶</a></h2>
<p>The bulk of my time “doing data science” is data cleaning and
pre-processing. Actually fitting an estimator or making predictions is a
relatively small proportion of the work.</p>
<p>You could manually do all your data-processing tasks as a sequence of
function calls starting with the raw data. Or, you could use
<cite>scikit-learn’s
“Pipeline`</cite> &lt;<a class="reference external" href="http://scikit-learn.org/stable/modules/pipeline.html">http://scikit-learn.org/stable/modules/pipeline.html</a>&gt;`__
to accomplish this and then some. <code class="docutils literal"><span class="pre">Pipeline</span></code>s offer a few advantages
over the manual solution.</p>
<p>First, your entire modeling process from raw data to final output is in
a self-contained object. No more wondering “did I remember to scale this
version of my model?” It’s there in the <code class="docutils literal"><span class="pre">Pipeline</span></code> for you to check.</p>
<p>Second, <code class="docutils literal"><span class="pre">Pipeline</span></code>s combine well with scikit-learn’s model selection
utilties, specifically <code class="docutils literal"><span class="pre">GridSearchCV</span></code> and <code class="docutils literal"><span class="pre">RandomizedSearchCV</span></code>.
You’re able to search over hyperparameters of the pipeline stages, just
like you would for an estimator.</p>
<p>Third, <code class="docutils literal"><span class="pre">Pipeline</span></code>s help prevent leaking information from your test
and validation sets to your training set. A common mistake is to compute
some pre-processing statistic on the <em>entire</em> dataset (before you’ve
train-test split) rather than just the training set. For example, you
might normalize a column by the average of all the observations. These
types of errors can lead you overestimate the performance of your model
on new observations.</p>
<p>Since dask-glm follows the scikit-learn API, we can reuse scikit-learn’s
<code class="docutils literal"><span class="pre">Pipeline</span></code> machinery, <em>with a few caveats.</em></p>
<p>Many of the tranformers built into scikit-learn will validate their
inputs. As part of this, array-like things are cast to numpy arrays.
Since dask-arrays are array-like they are converted and things “work”,
but this might not be ideal when your dataset doesn’t fit in memory.</p>
<p>Second, some things are just fundamentally hard to do on large datasets.
For example, naively dummy-encoding a dataset requires a full scan of
the data to determine the set of unique values per categorical column.
When your dataset fits in memory, this isn’t a huge deal. But when it’s
scattered across a cluster, this could become a bottleneck.</p>
<p>If you know the set of possible values <em>ahead</em> of time, you can do much
better. You can encode the categorical columns as pandas
<code class="docutils literal"><span class="pre">Categoricals</span></code>, and then convert with <code class="docutils literal"><span class="pre">get_dummies</span></code>, without having
to do an expensive full-scan, just to compute the set of unique values.
We’ll do that on the <code class="docutils literal"><span class="pre">VendorID</span></code> and <code class="docutils literal"><span class="pre">payment_type</span></code> columnms.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="k">import</span> <span class="n">TransformerMixin</span><span class="p">,</span> <span class="n">BaseEstimator</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="k">import</span> <span class="n">make_pipeline</span>
</pre></div>
</div>
</div>
<p>First let’s write a little transformer to convert columns to
<code class="docutils literal"><span class="pre">Categoricals</span></code>. If you aren’t familar with scikit-learn transformers,
the basic idea is that the transformer must implement two methods:
<code class="docutils literal"><span class="pre">.fit</span></code> and <code class="docutils literal"><span class="pre">.tranform</span></code>.</p>
<p><code class="docutils literal"><span class="pre">.fit</span></code> is called during training. It learns something about the data
and records it on <code class="docutils literal"><span class="pre">self</span></code>.</p>
<p>Then <code class="docutils literal"><span class="pre">.transform</span></code> uses what’s learned during <code class="docutils literal"><span class="pre">.fit</span></code> to transform the
feature matrix somehow.</p>
<p>A <code class="docutils literal"><span class="pre">Pipeline</span></code> is simply a chain of transformers, each one is <code class="docutils literal"><span class="pre">fit</span></code> on
some data, and passes the output of <code class="docutils literal"><span class="pre">.transform</span></code> onto the next step;
the final output is an <code class="docutils literal"><span class="pre">Estimator</span></code>, like <code class="docutils literal"><span class="pre">LogisticRegression</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">CategoricalEncoder</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Encode `categories` as pandas `Categorical`</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    categories : Dict[str, list]</span>
<span class="sd">        Mapping from column name to list of possible values</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">categories</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">categories</span> <span class="o">=</span> <span class="n">categories</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># &quot;stateless&quot; transformer. Don&#39;t have anything to learn here</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">column</span><span class="p">,</span> <span class="n">categories</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">categories</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">X</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;category&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">cat</span><span class="o">.</span><span class="n">set_categories</span><span class="p">(</span><span class="n">categories</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X</span>
</pre></div>
</div>
</div>
<p>We’ll also want a daskified version of scikit-learn’s
<code class="docutils literal"><span class="pre">StandardScaler</span></code>, that won’t eagerly convert a dask.array to a numpy
array (N.B. the scikit-learn version has more features and error
handling, but this will work for now).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">class</span> <span class="nc">StandardScaler</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">with_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_std</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">columns</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">with_mean</span> <span class="o">=</span> <span class="n">with_mean</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">with_std</span> <span class="o">=</span> <span class="n">with_std</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">columns</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">columns_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">columns_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">columns</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_mean</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">columns_</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_std</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">columns_</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_mean</span><span class="p">:</span>
            <span class="n">X</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">columns_</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">columns_</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">with_std</span><span class="p">:</span>
            <span class="n">X</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">columns_</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">columns_</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_</span>
        <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
<p>Finally, I’ve written a dummy encoder transformer that converts
categoricals to dummy-encoded interger columns. The full implementation
is a bit long for a blog post, but you can see it
<a class="reference external" href="https://github.com/TomAugspurger/sktransformers/blob/master/sktransformers/preprocessing.py#L77">here</a>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">dummy_encoder</span> <span class="k">import</span> <span class="n">DummyEncoder</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">CategoricalEncoder</span><span class="p">({</span><span class="s2">&quot;VendorID&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                        <span class="s2">&quot;payment_type&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]}),</span>
    <span class="n">DummyEncoder</span><span class="p">(),</span>
    <span class="n">StandardScaler</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;passenger_count&#39;</span><span class="p">,</span> <span class="s1">&#39;trip_distance&#39;</span><span class="p">,</span> <span class="s1">&#39;fare_amount&#39;</span><span class="p">]),</span>
    <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>So that’s our pipeline. We can go ahead and fit it just like before,
passing in the raw data.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="o">%%</span><span class="k">time</span>
pipe.fit(X_train, y_train.values)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 5min 24s, sys: 42.4 s, total: 6min 6s
Wall time: 37min 4s
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[18]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>Pipeline(memory=None,
     steps=[(&#39;categoricalencoder&#39;, CategoricalEncoder(categories={&#39;VendorID&#39;: [1, 2], &#39;payment_type&#39;: [1, 2, 3, 4, 5]})), (&#39;dummyencoder&#39;, DummyEncoder(columns=None, drop_first=False)), (&#39;standardscaler&#39;, StandardScaler(columns=[&#39;passenger_count&#39;, &#39;trip_distance&#39;, &#39;fare_amount&#39;],
        with_mean=True, ...iter=100, over_relax=1, regularizer=&#39;l2&#39;, reltol=0.01, rho=1,
          solver=&#39;admm&#39;, tol=0.0001))])
</pre></div>
</div>
</div>
<p>And we can score it as well. The <code class="docutils literal"><span class="pre">Pipeline</span></code> ensures that all of the
nescessary transformations take place before calling the estimator’s
<code class="docutils literal"><span class="pre">score</span></code> method.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">pipe</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[19]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.97890756758465358
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">pipe</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[20]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.97888495550125487
</pre></div>
</div>
</div>
</div>
<div class="section" id="Grid-Search">
<h2>Grid Search<a class="headerlink" href="#Grid-Search" title="Permalink to this headline">¶</a></h2>
<p>As explained earlier, Pipelines and grid search go hand-in-hand. Let’s
run a quick example with
<a class="reference external" href="http://dask-searchcv.readthedocs.io/en/latest/">dask-searchcv</a>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">GridSearchCV</span>
<span class="kn">import</span> <span class="nn">dask_searchcv</span> <span class="k">as</span> <span class="nn">dcv</span>
</pre></div>
</div>
</div>
<p>We’ll search over two hyperparameters</p>
<ol class="arabic simple">
<li>Whether or not to standardize the variance of each column in
<code class="docutils literal"><span class="pre">StandardScaler</span></code></li>
<li>The strength of the regularization in <code class="docutils literal"><span class="pre">LogisticRegression</span></code></li>
</ol>
<p>This involves fitting many models, one for each combination of
paramters. dask-searchcv is smart enough to know that early stages in
the pipeline (like the categorical and dummy encoding) are shared among
all the combinations, and so only fits them once.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;standardscaler__with_std&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">],</span>
    <span class="s1">&#39;logisticregression__lamduh&#39;</span><span class="p">:</span> <span class="p">[</span><span class="o">.</span><span class="mi">001</span><span class="p">,</span> <span class="o">.</span><span class="mi">01</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="p">}</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">CategoricalEncoder</span><span class="p">({</span><span class="s2">&quot;VendorID&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                        <span class="s2">&quot;payment_type&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]}),</span>
    <span class="n">DummyEncoder</span><span class="p">(),</span>
    <span class="n">StandardScaler</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;passenger_count&#39;</span><span class="p">,</span> <span class="s1">&#39;trip_distance&#39;</span><span class="p">,</span> <span class="s1">&#39;fare_amount&#39;</span><span class="p">]),</span>
    <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">gs</span> <span class="o">=</span> <span class="n">dcv</span><span class="o">.</span><span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="o">%%</span><span class="k">time</span>
gs.fit(X_train, y_train.values)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
CPU times: user 1min 5s, sys: 24 s, total: 1min 29s
Wall time: 40min 24s
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[23]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>GridSearchCV(cache_cv=True, cv=None, error_score=&#39;raise&#39;,
       estimator=Pipeline(memory=None,
     steps=[(&#39;categoricalencoder&#39;, CategoricalEncoder(categories={&#39;VendorID&#39;: [1, 2], &#39;payment_type&#39;: [1, 2, 3, 4, 5]})), (&#39;dummyencoder&#39;, DummyEncoder(columns=None, drop_first=False)), (&#39;standardscaler&#39;, StandardScaler(columns=[&#39;passenger_count&#39;, &#39;trip_distance&#39;, &#39;fare_amount&#39;],
        with_mean=True, ...iter=100, over_relax=1, regularizer=&#39;l2&#39;, reltol=0.01, rho=1,
          solver=&#39;admm&#39;, tol=0.0001))]),
       get=None, iid=True,
       param_grid={&#39;standardscaler__with_std&#39;: [True, False], &#39;logisticregression__lamduh&#39;: [0.001, 0.01, 0.1, 1]},
       refit=True, return_train_score=True, scoring=None)
</pre></div>
</div>
</div>
<p>Now we have access to the usual attributes like <code class="docutils literal"><span class="pre">cv_results_</span></code> learned
by the grid search object:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [31]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">gs</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[31]:
</pre></div>
</div>
<div class="output_area docutils container">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean_test_score</th>
      <th>mean_train_score</th>
      <th>param_logisticregression__lamduh</th>
      <th>param_standardscaler__with_std</th>
      <th>params</th>
      <th>rank_test_score</th>
      <th>split0_test_score</th>
      <th>split0_train_score</th>
      <th>split1_test_score</th>
      <th>split1_train_score</th>
      <th>split2_test_score</th>
      <th>split2_train_score</th>
      <th>std_test_score</th>
      <th>std_train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.946754</td>
      <td>0.946304</td>
      <td>0.001</td>
      <td>True</td>
      <td>{'logisticregression__lamduh': 0.001, 'standar...</td>
      <td>1</td>
      <td>0.978945</td>
      <td>0.978819</td>
      <td>0.882557</td>
      <td>0.881177</td>
      <td>0.978758</td>
      <td>0.978918</td>
      <td>0.045394</td>
      <td>0.046052</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.946754</td>
      <td>0.946304</td>
      <td>0.001</td>
      <td>False</td>
      <td>{'logisticregression__lamduh': 0.001, 'standar...</td>
      <td>1</td>
      <td>0.978945</td>
      <td>0.978819</td>
      <td>0.882557</td>
      <td>0.881177</td>
      <td>0.978758</td>
      <td>0.978918</td>
      <td>0.045394</td>
      <td>0.046052</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.946754</td>
      <td>0.946304</td>
      <td>0.01</td>
      <td>True</td>
      <td>{'logisticregression__lamduh': 0.01, 'standard...</td>
      <td>1</td>
      <td>0.978945</td>
      <td>0.978819</td>
      <td>0.882557</td>
      <td>0.881177</td>
      <td>0.978758</td>
      <td>0.978918</td>
      <td>0.045394</td>
      <td>0.046052</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.946754</td>
      <td>0.946304</td>
      <td>0.01</td>
      <td>False</td>
      <td>{'logisticregression__lamduh': 0.01, 'standard...</td>
      <td>1</td>
      <td>0.978945</td>
      <td>0.978819</td>
      <td>0.882557</td>
      <td>0.881177</td>
      <td>0.978758</td>
      <td>0.978918</td>
      <td>0.045394</td>
      <td>0.046052</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.946754</td>
      <td>0.946304</td>
      <td>0.1</td>
      <td>True</td>
      <td>{'logisticregression__lamduh': 0.1, 'standards...</td>
      <td>1</td>
      <td>0.978945</td>
      <td>0.978819</td>
      <td>0.882557</td>
      <td>0.881177</td>
      <td>0.978758</td>
      <td>0.978918</td>
      <td>0.045394</td>
      <td>0.046052</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.946754</td>
      <td>0.946304</td>
      <td>0.1</td>
      <td>False</td>
      <td>{'logisticregression__lamduh': 0.1, 'standards...</td>
      <td>1</td>
      <td>0.978945</td>
      <td>0.978819</td>
      <td>0.882557</td>
      <td>0.881177</td>
      <td>0.978758</td>
      <td>0.978918</td>
      <td>0.045394</td>
      <td>0.046052</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.946754</td>
      <td>0.946304</td>
      <td>1</td>
      <td>True</td>
      <td>{'logisticregression__lamduh': 1, 'standardsca...</td>
      <td>1</td>
      <td>0.978945</td>
      <td>0.978819</td>
      <td>0.882557</td>
      <td>0.881177</td>
      <td>0.978758</td>
      <td>0.978918</td>
      <td>0.045394</td>
      <td>0.046052</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.946754</td>
      <td>0.946304</td>
      <td>1</td>
      <td>False</td>
      <td>{'logisticregression__lamduh': 1, 'standardsca...</td>
      <td>1</td>
      <td>0.978945</td>
      <td>0.978819</td>
      <td>0.882557</td>
      <td>0.881177</td>
      <td>0.978758</td>
      <td>0.978918</td>
      <td>0.045394</td>
      <td>0.046052</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>And we can do our usual checks on model fit for the training set:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">gs</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[25]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.97888698720008394
</pre></div>
</div>
</div>
<p>And the test set:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [26]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">gs</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[26]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>0.97886801935289736
</pre></div>
</div>
</div>
<p>Hopefully your reaction to everything here is somewhere between a
nodding head and a yawn. If you’re familiar with scikit-learn,
everything here should look pretty routine. It’s the same API you know
and love, scaled out to larger datasets thanks to dask-glm.</p>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="xgboost.html" class="btn btn-neutral float-right" title="Dask and XGBoost" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="predict.html" class="btn btn-neutral" title="Out-of-core Prediction" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Dask developers.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>