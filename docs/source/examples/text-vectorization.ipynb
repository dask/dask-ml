{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vectorization Example\n",
    "\n",
    "This example illustrates how Dask-ML can be used to vectorize textual data in parallel.\n",
    "This example is adapted frmo https://github.com/scikit-learn/scikit-learn/tree/master/examples/applications/plot_out_of_core_classification.py#L143."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "from glob import glob\n",
    "\n",
    "import scipy.sparse\n",
    "from sklearn.datasets import get_data_home\n",
    "from sklearn.externals.six.moves.urllib.request import urlretrieve\n",
    "\n",
    "import dask\n",
    "import dask.bag as db\n",
    "\n",
    "from dask_ml.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_reuters(data_path=None):\n",
    "    \"\"\"Fetch documents of the Reuters dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    DOWNLOAD_URL = ('http://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "                    'reuters21578-mld/reuters21578.tar.gz')\n",
    "    ARCHIVE_FILENAME = 'reuters21578.tar.gz'\n",
    "\n",
    "    if data_path is None:\n",
    "        data_path = os.path.join(get_data_home(), \"reuters\")\n",
    "    if not os.path.exists(data_path):\n",
    "        \"\"\"Download the dataset.\"\"\"\n",
    "        print(\"downloading dataset (once and for all) into %s\" %\n",
    "              data_path)\n",
    "        os.mkdir(data_path)\n",
    "\n",
    "        def progress(blocknum, bs, size):\n",
    "            total_sz_mb = '%.2f MB' % (size / 1e6)\n",
    "            current_sz_mb = '%.2f MB' % ((blocknum * bs) / 1e6)\n",
    "            print('\\rdownloaded %s / %s' % (current_sz_mb, total_sz_mb))\n",
    "\n",
    "        archive_path = os.path.join(data_path, ARCHIVE_FILENAME)\n",
    "        urlretrieve(DOWNLOAD_URL, filename=archive_path,\n",
    "                    reporthook=progress)\n",
    "        print('\\r')\n",
    "        print(\"untarring Reuters dataset...\")\n",
    "        tarfile.open(archive_path, 'r:gz').extractall(data_path)\n",
    "        print(\"done.\")\n",
    "    return data_path\n",
    "\n",
    "\n",
    "def load_from_filename(file_path):\n",
    "    with open(file_path, 'rb') as fh:\n",
    "        txt = fh.read().decode('latin-1')\n",
    "\n",
    "    return re.findall('(?<=<BODY>)[^<]+(?=</BODY>)', txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = fetch_reuters()\n",
    "files = glob(os.path.join(data_path, 'reut2*'))\n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`files` is a list of filepaths. We can build a Dask Bag that will (lazily) read in the contents of these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (db.from_sequence(files)\n",
    "          .map(load_from_filename)\n",
    "          .flatten())\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element of `text` is a single article. Here's the first 100 characters from the first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text.take(1)[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API is the same as with scikit-learn. You instantiate the estimator, and pass the data to `fit` or `fit_transform`.\n",
    "Only in this case the data is a `dask.bag.Bag` or `dask.dataframe.Series`.\n",
    "Transfomration happens in parallel, and returns a dask Array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = HashingVectorizer()\n",
    "X = vect.fit_transform(text)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each block of the dask array contains a scipy sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.blocks[0].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy sparse matrics don't meet the full ndarray interface, so while you can *store* them in a Dask Array, many operations on a Dask Array composed of SciPy sparse matricies will fail. The [`sparse`](http://sparse.pydata.org/en/latest/) project implements an n-dimensional sparse array conforms to NumPy's ndarray interface.\n",
    "\n",
    "In this case, we'll convert it to a a sparse COO array, and then call `compute`, materializing the result as a single `COO` array. For this dataset, that's only about 24 MB. For large datasets, you would want to continue processing the data as a Dask array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparse\n",
    "\n",
    "X.map_blocks(sparse.COO.from_scipy_sparse).compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
