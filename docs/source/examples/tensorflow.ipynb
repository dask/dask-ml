{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask and Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\" align=\"left\" width=\"30%\" alt=\"Dask logo\">\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/_static/images/tensorflow/logo.png\" align=\"left\" width=\"15%\" alt=\"Dask logo\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See http://matthewrocklin.com/blog/work/2017/02/11/dask-tensorflow for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask import delayed\n",
    "from dask_tensorflow import start_tensorflow\n",
    "from distributed import Client, progress, get_worker, worker_client, Queue\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=3, memory_limit='2GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist():\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist = input_data.read_data_sets('/tmp/mnist-data', one_hot=True)\n",
    "    return mnist.train.images, mnist.train.labels\n",
    "\n",
    "get_mnist()  # Download the data ahead of time\n",
    "\n",
    "# Scale up: increase the number of copies of the dataset\n",
    "n_copies = 2\n",
    "datasets = [delayed(get_mnist)() for i in range(n_copies)]\n",
    "\n",
    "images = [d[0] for d in datasets]\n",
    "labels = [d[1] for d in datasets]\n",
    "\n",
    "images = [da.from_delayed(im, shape=(55000, 784), dtype='float32') for im in images]\n",
    "labels = [da.from_delayed(la, shape=(55000, 10), dtype='float32') for la in labels]\n",
    "\n",
    "images = da.concatenate(images, axis=0)\n",
    "labels = da.concatenate(labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = client.persist([images, labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = images[1].compute().reshape((28, 28))\n",
    "plt.imshow(im, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = images.mean(axis=0).compute().reshape((28, 28))\n",
    "plt.imshow(im, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.rechunk((10000, 784))\n",
    "labels = labels.rechunk((10000, 10))\n",
    "\n",
    "images = images.to_delayed().flatten().tolist()\n",
    "labels = labels.to_delayed().flatten().tolist()\n",
    "batches = [delayed([im, la]) for im, la in zip(images, labels)]\n",
    "\n",
    "batches = client.compute(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_spec, dask_spec = start_tensorflow(client, ps=1, worker=1, scorer=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tempfile\n",
    "import time\n",
    "from queue import Empty\n",
    "\n",
    "IMAGE_PIXELS = 28\n",
    "hidden_units = 100\n",
    "learning_rate = 0.01\n",
    "sync_replicas = False\n",
    "replicas_to_aggregate = len(dask_spec['worker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(server):\n",
    "    worker_device = \"/job:%s/task:%d\" % (server.server_def.job_name,\n",
    "                                         server.server_def.task_index)\n",
    "    task_index = server.server_def.task_index\n",
    "    is_chief = task_index == 0\n",
    "\n",
    "    with tf.device(tf.train.replica_device_setter(\n",
    "                      worker_device=worker_device,\n",
    "                      ps_device=\"/job:ps/cpu:0\",\n",
    "                      cluster=tf_spec)):\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "        # Variables of the hidden layer\n",
    "        hid_w = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [IMAGE_PIXELS * IMAGE_PIXELS, hidden_units],\n",
    "                stddev=1.0 / IMAGE_PIXELS),\n",
    "            name=\"hid_w\")\n",
    "        hid_b = tf.Variable(tf.zeros([hidden_units]), name=\"hid_b\")\n",
    "\n",
    "        # Variables of the softmax layer\n",
    "        sm_w = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [hidden_units, 10],\n",
    "                stddev=1.0 / math.sqrt(hidden_units)),\n",
    "            name=\"sm_w\")\n",
    "        sm_b = tf.Variable(tf.zeros([10]), name=\"sm_b\")\n",
    "\n",
    "        # Ops: located on the worker specified with task_index\n",
    "        x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS])\n",
    "        y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "        hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\n",
    "        hid = tf.nn.relu(hid_lin)\n",
    "\n",
    "        y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n",
    "        cross_entropy = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n",
    "\n",
    "        opt = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        if sync_replicas:\n",
    "            if replicas_to_aggregate is None:\n",
    "                replicas_to_aggregate = num_workers\n",
    "            else:\n",
    "                replicas_to_aggregate = replicas_to_aggregate\n",
    "\n",
    "            opt = tf.train.SyncReplicasOptimizer(\n",
    "                      opt,\n",
    "                      replicas_to_aggregate=replicas_to_aggregate,\n",
    "                      total_num_replicas=num_workers,\n",
    "                      name=\"mnist_sync_replicas\")\n",
    "\n",
    "        train_step = opt.minimize(cross_entropy, global_step=global_step)\n",
    "\n",
    "        if sync_replicas:\n",
    "            local_init_op = opt.local_step_init_op\n",
    "            if is_chief:\n",
    "                local_init_op = opt.chief_init_op\n",
    "\n",
    "            ready_for_local_init_op = opt.ready_for_local_init_op\n",
    "\n",
    "            # Initial token and chief queue runners required by the sync_replicas mode\n",
    "            chief_queue_runner = opt.get_chief_queue_runner()\n",
    "            sync_init_op = opt.get_init_tokens_op()\n",
    "\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        train_dir = tempfile.mkdtemp()\n",
    "\n",
    "        if sync_replicas:\n",
    "          sv = tf.train.Supervisor(\n",
    "              is_chief=is_chief,\n",
    "              logdir=train_dir,\n",
    "              init_op=init_op,\n",
    "              local_init_op=local_init_op,\n",
    "              ready_for_local_init_op=ready_for_local_init_op,\n",
    "              recovery_wait_secs=1,\n",
    "              global_step=global_step)\n",
    "        else:\n",
    "          sv = tf.train.Supervisor(\n",
    "              is_chief=is_chief,\n",
    "              logdir=train_dir,\n",
    "              init_op=init_op,\n",
    "              recovery_wait_secs=1,\n",
    "              global_step=global_step)\n",
    "\n",
    "        sess_config = tf.ConfigProto(\n",
    "            allow_soft_placement=True,\n",
    "            log_device_placement=False,\n",
    "            device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % task_index])\n",
    "\n",
    "        # The chief worker (task_index==0) session will prepare the session,\n",
    "        # while the remaining workers will wait for the preparation to complete.\n",
    "        if is_chief:\n",
    "          print(\"Worker %d: Initializing session...\" % task_index)\n",
    "        else:\n",
    "          print(\"Worker %d: Waiting for session to be initialized...\" %\n",
    "                task_index)\n",
    "\n",
    "        sess = sv.prepare_or_wait_for_session(server.target, config=sess_config)\n",
    "\n",
    "        if sync_replicas and is_chief:\n",
    "          # Chief worker will start the chief queue runner and call the init op.\n",
    "          sess.run(sync_init_op)\n",
    "          sv.start_queue_runners(sess, [chief_queue_runner])\n",
    "\n",
    "        return sess, x, y_, train_step, global_step, cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ps_task():\n",
    "    worker = get_worker()\n",
    "    worker.tensorflow_server.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_task():\n",
    "    with worker_client() as c:\n",
    "        # Scores Channel\n",
    "        scores = Queue('scores')\n",
    "\n",
    "        # Make Model\n",
    "        worker = get_worker()\n",
    "        server = worker.tensorflow_server\n",
    "        sess, x, y_, _, _, cross_entropy = model(server)\n",
    "\n",
    "        # Testing Data\n",
    "        from tensorflow.examples.tutorials.mnist import input_data\n",
    "        mnist = input_data.read_data_sets('/tmp/mnist-data', one_hot=True)\n",
    "        test_data = {x: mnist.validation.images,\n",
    "                     y_: mnist.validation.labels}\n",
    "\n",
    "        # Main Loop\n",
    "        while True:\n",
    "            score = sess.run(cross_entropy, feed_dict=test_data)\n",
    "            scores.put(float(score))\n",
    "\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_task():\n",
    "    with worker_client() as c:\n",
    "        scores = Queue('scores')\n",
    "        num_workers = replicas_to_aggregate = len(dask_spec['worker'])\n",
    "\n",
    "        worker = get_worker()\n",
    "        server = worker.tensorflow_server\n",
    "        tf_queue = worker.tensorflow_queue\n",
    "\n",
    "        # Make model\n",
    "        sess, x, y_, train_step, global_step, _= model(server)\n",
    "\n",
    "        # Main loop\n",
    "        sc = scores.get()\n",
    "        while not scores or sc > 1000:\n",
    "            try:\n",
    "                batch = tf_queue.get(timeout=0.5)\n",
    "            except Empty:\n",
    "                continue\n",
    "\n",
    "            train_data = {x: batch[0],\n",
    "                          y_: batch[1]}\n",
    "\n",
    "            sess.run([train_step, global_step], feed_dict=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_tasks = [client.submit(ps_task, workers=worker)\n",
    "            for worker in dask_spec['ps']]\n",
    "\n",
    "worker_tasks = [client.submit(worker_task, workers=addr, pure=False)\n",
    "                for addr in dask_spec['worker']]\n",
    "\n",
    "scorer_task = client.submit(scoring_task, workers=dask_spec['scorer'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_dask_to_tensorflow(batch):\n",
    "    worker = get_worker()\n",
    "    worker.tensorflow_queue.put(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = Queue('scores')\n",
    "score = scores.get()\n",
    "while score > 1000:\n",
    "    print(score)\n",
    "    dump = client.map(transfer_dask_to_tensorflow, batches,\n",
    "                      workers=dask_spec['worker'], pure=False)\n",
    "    score = scores.get()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
