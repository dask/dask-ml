{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`dask-glm`](https://github.com/dask/dask-glm) is a library for fitting generalized linear models on large datasets.\n",
    "The heart of the project is the set of optimization routines that work on either NumPy or dask arrays.\n",
    "See [these](https://mrocklin.github.com/blog/work/2017/03/22/dask-glm-1) [two](http://matthewrocklin.com/blog/work/2017/04/19/dask-glm-2) blogposts describing how dask-glm works internally.\n",
    "\n",
    "This notebook is shows an example of the higher-level scikit-learn style API built on top of these optimization routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from distributed import Client\n",
    "\n",
    "from dask import persist, compute\n",
    "from dask_glm.estimators import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll setup a [`distributed.Client`](http://distributed.readthedocs.io/en/latest/api.html#distributed.client.Client) locally. In the real world you could connect to a cluster of dask-workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration, we'll use the perennial NYC taxi cab dataset.\n",
    "Since I'm just running things on my laptop, we'll just grab the first month's worth of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('trip.csv'):\n",
    "    s3 = S3FileSystem(anon=True)\n",
    "    s3.get(\"dask-data/nyc-taxi/2015/yellow_tripdata_2015-01.csv\", \"trip.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddf = dd.read_csv(\"trip.csv\")\n",
    "ddf = ddf.repartition(npartitions=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I happen to know that some of the values in this dataset are suspect, so let's drop them.\n",
    "Scikit-learn doesn't support filtering observations inside a pipeline ([yet](https://github.com/scikit-learn/scikit-learn/issues/3855)), so we'll do this before anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these filter out less than 1% of the observations\n",
    "ddf = ddf[(ddf.trip_distance < 20) &\n",
    "          (ddf.fare_amount < 150)]\n",
    "ddf = ddf.repartition(npartitions=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll split our DataFrame into a train and test set, and select our feature matrix and target column (whether the passenger tipped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train, df_test = ddf.random_split([0.8, 0.2], random_state=2)\n",
    "\n",
    "columns = ['VendorID', 'passenger_count', 'trip_distance', 'payment_type', 'fare_amount']\n",
    "\n",
    "X_train, y_train = df_train[columns], df_train['tip_amount'] > 0\n",
    "X_test, y_test = df_test[columns], df_test['tip_amount'] > 0\n",
    "\n",
    "X_train, y_train, X_test, y_test = persist(\n",
    "    X_train, y_train, X_test, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID  passenger_count  trip_distance  payment_type  fare_amount\n",
       "2         1                1            1.8             2          9.5\n",
       "3         1                1            0.5             2          3.5\n",
       "4         1                1            3.0             2         15.0\n",
       "5         1                1            9.0             1         27.0\n",
       "6         1                1            2.2             2         14.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "5     True\n",
       "6    False\n",
       "Name: tip_amount, dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,155,301 observations\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(X_train):,d} observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our training data in hand, we fit our logistic regression.\n",
    "Nothing here should be surprising to those familiar with `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 27s, sys: 13.3 s, total: 1min 40s\n",
      "Wall time: 11min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# this is a *dask-glm* LogisticRegresion, not scikit-learn\n",
    "lm = LogisticRegression(fit_intercept=False)\n",
    "lm.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, following the lead of scikit-learn we can measure the performance of the estimator on the training dataset using the `.score` method.\n",
    "For LogisticRegression this is the mean accuracy score (what percent of the predicted matched the actual)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 205 ms, sys: 25 ms, total: 230 ms\n",
      "Wall time: 364 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.88082578743850137"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lm.score(X_train.values, y_train.values).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 144 ms, sys: 21.8 ms, total: 166 ms\n",
      "Wall time: 249 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.88061000067744588"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lm.score(X_test.values, y_test.values).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "The bulk of my time \"doing data science\" is data cleaning and pre-processing.\n",
    "Actually fitting an estimator or making predictions is a relatively small proportion of the work.\n",
    "\n",
    "You could manually do all your data-processing tasks as a sequence of function calls starting with the raw data.\n",
    "Or, you could use [scikit-learn's `Pipeline`](http://scikit-learn.org/stable/modules/pipeline.html) to accomplish this and then some.\n",
    "`Pipeline`s offer a few advantages over the manual solution.\n",
    "\n",
    "First, your entire modeling process from raw data to final output is in a self-contained object. No more wondering \"did I remember to scale this version of my model?\" It's there in the `Pipeline` for you to check.\n",
    "\n",
    "Second, `Pipeline`s combine well with scikit-learn's model selection utilties, specifically `GridSearchCV` and `RandomizedSearchCV`. You're able to search over hyperparameters of the pipeline stages, just like you would for an estimator.\n",
    "\n",
    "Third, `Pipeline`s help prevent leaking information from your test and validation sets to your training set.\n",
    "A common mistake is to compute some pre-processing statistic on the *entire* dataset (before you've train-test split) rather than just the training set. For example, you might normalize a column by the average of all the observations.\n",
    "These types of errors can lead you overestimate the performance of your model on new observations.\n",
    "\n",
    "Since dask-glm follows the scikit-learn API, we can reuse scikit-learn's `Pipeline` machinery, *with a few caveats.*\n",
    "\n",
    "Many of the tranformers built into scikit-learn will validate their inputs. As part of this,\n",
    "array-like things are cast to numpy arrays. Since dask-arrays are array-like they are converted\n",
    "and things \"work\", but this might not be ideal when your dataset doesn't fit in memory.\n",
    "\n",
    "Second, some things are just fundamentally hard to do on large datasets.\n",
    "For example, naively dummy-encoding a dataset requires a full scan of the data to determine the set of unique values per categorical column.\n",
    "When your dataset fits in memory, this isn't a huge deal. But when it's scattered across a cluster, this could become\n",
    "a bottleneck.\n",
    "\n",
    "If you know the set of possible values *ahead* of time, you can do much better.\n",
    "You can encode the categorical columns as pandas `Categoricals`, and then convert with `get_dummies`, without having to do an expensive full-scan, just to compute the set of unique values.\n",
    "We'll do that on the `VendorID` and `payment_type` columnms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's write a little transformer to convert columns to `Categoricals`.\n",
    "If you aren't familar with scikit-learn transformers, the basic idea is that the transformer must implement two methods: `.fit` and `.tranform`.\n",
    "\n",
    "`.fit` is called during training.\n",
    "It learns something about the data and records it on `self`.\n",
    "\n",
    "Then `.transform` uses what's learned during `.fit` to transform the feature matrix somehow.\n",
    "\n",
    "A `Pipeline` is simply a chain of transformers, each one is `fit` on some data, and passes the output of `.transform` onto the next step; the final output is an `Estimator`, like `LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Encode `categories` as pandas `Categorical`\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    categories : Dict[str, list]\n",
    "        Mapping from column name to list of possible values\n",
    "    \"\"\"\n",
    "    def __init__(self, categories):\n",
    "        self.categories = categories\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # \"stateless\" transformer. Don't have anything to learn here\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        for column, categories in self.categories.items():\n",
    "            X[column] = X[column].astype('category').cat.set_categories(categories)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also want a daskified version of scikit-learn's `StandardScaler`, that won't eagerly\n",
    "convert a dask.array to a numpy array (N.B. the scikit-learn version has more features and error handling, but this will work for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StandardScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None, with_mean=True, with_std=True):\n",
    "        self.columns = columns\n",
    "        self.with_mean = with_mean\n",
    "        self.with_std = with_std\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.columns is None:\n",
    "            self.columns_ = X.columns\n",
    "        else:\n",
    "            self.columns_ = self.columns\n",
    "        if self.with_mean:\n",
    "            self.mean_ = X[self.columns_].mean(0)\n",
    "        if self.with_std:\n",
    "            self.scale_ = X[self.columns_].std(0)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        if self.with_mean:\n",
    "            X[self.columns_] = X[self.columns_] - self.mean_\n",
    "        if self.with_std:\n",
    "            X[self.columns_] = X[self.columns_] / self.scale_\n",
    "        return X.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I've written a dummy encoder transformer that converts categoricals\n",
    "to dummy-encoded interger columns. The full implementation is a bit long for a blog post, but you can see it [here](https://github.com/TomAugspurger/sktransformers/blob/master/sktransformers/preprocessing.py#L77)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dummy_encoder import DummyEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(\n",
    "    CategoricalEncoder({\"VendorID\": [1, 2],\n",
    "                        \"payment_type\": [1, 2, 3, 4, 5]}),\n",
    "    DummyEncoder(),\n",
    "    StandardScaler(columns=['passenger_count', 'trip_distance', 'fare_amount']),\n",
    "    LogisticRegression(fit_intercept=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's our pipeline.\n",
    "We can go ahead and fit it just like before, passing in the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 24s, sys: 42.4 s, total: 6min 6s\n",
      "Wall time: 37min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('categoricalencoder', CategoricalEncoder(categories={'VendorID': [1, 2], 'payment_type': [1, 2, 3, 4, 5]})), ('dummyencoder', DummyEncoder(columns=None, drop_first=False)), ('standardscaler', StandardScaler(columns=['passenger_count', 'trip_distance', 'fare_amount'],\n",
       "        with_mean=True, ...iter=100, over_relax=1, regularizer='l2', reltol=0.01, rho=1,\n",
       "          solver='admm', tol=0.0001))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pipe.fit(X_train, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can score it as well. The `Pipeline` ensures that all of the nescessary transformations take place before calling the estimator's `score` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97890756758465358"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_train, y_train.values).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97888495550125487"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_test, y_test.values).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "As explained earlier, Pipelines and grid search go hand-in-hand.\n",
    "Let's run a quick example with [dask-searchcv](http://dask-searchcv.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import dask_searchcv as dcv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll search over two hyperparameters\n",
    "\n",
    "1. Whether or not to standardize the variance of each column in `StandardScaler`\n",
    "2. The strength of the regularization in `LogisticRegression`\n",
    "\n",
    "This involves fitting many models, one for each combination of paramters.\n",
    "dask-searchcv is smart enough to know that early stages in the pipeline (like the categorical and dummy encoding) are shared among all the combinations, and so only fits them once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'standardscaler__with_std': [True, False],\n",
    "    'logisticregression__lamduh': [.001, .01, .1, 1],\n",
    "}\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    CategoricalEncoder({\"VendorID\": [1, 2],\n",
    "                        \"payment_type\": [1, 2, 3, 4, 5]}),\n",
    "    DummyEncoder(),\n",
    "    StandardScaler(columns=['passenger_count', 'trip_distance', 'fare_amount']),\n",
    "    LogisticRegression(fit_intercept=False)\n",
    ")\n",
    "\n",
    "gs = dcv.GridSearchCV(pipe, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 5s, sys: 24 s, total: 1min 29s\n",
      "Wall time: 40min 24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cache_cv=True, cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('categoricalencoder', CategoricalEncoder(categories={'VendorID': [1, 2], 'payment_type': [1, 2, 3, 4, 5]})), ('dummyencoder', DummyEncoder(columns=None, drop_first=False)), ('standardscaler', StandardScaler(columns=['passenger_count', 'trip_distance', 'fare_amount'],\n",
       "        with_mean=True, ...iter=100, over_relax=1, regularizer='l2', reltol=0.01, rho=1,\n",
       "          solver='admm', tol=0.0001))]),\n",
       "       iid=True,\n",
       "       param_grid={'standardscaler__with_std': [True, False], 'logisticregression__lamduh': [0.001, 0.01, 0.1, 1]},\n",
       "       refit=True, return_train_score=True, scoring=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "gs.fit(X_train, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have access to the usual attributes like `cv_results_` learned by the grid search object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_logisticregression__lamduh</th>\n",
       "      <th>param_standardscaler__with_std</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.946754</td>\n",
       "      <td>0.946304</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>{'logisticregression__lamduh': 0.001, 'standar...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.978945</td>\n",
       "      <td>0.978819</td>\n",
       "      <td>0.882557</td>\n",
       "      <td>0.881177</td>\n",
       "      <td>0.978758</td>\n",
       "      <td>0.978918</td>\n",
       "      <td>0.045394</td>\n",
       "      <td>0.046052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.946754</td>\n",
       "      <td>0.946304</td>\n",
       "      <td>0.001</td>\n",
       "      <td>False</td>\n",
       "      <td>{'logisticregression__lamduh': 0.001, 'standar...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.978945</td>\n",
       "      <td>0.978819</td>\n",
       "      <td>0.882557</td>\n",
       "      <td>0.881177</td>\n",
       "      <td>0.978758</td>\n",
       "      <td>0.978918</td>\n",
       "      <td>0.045394</td>\n",
       "      <td>0.046052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.946754</td>\n",
       "      <td>0.946304</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>{'logisticregression__lamduh': 0.01, 'standard...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.978945</td>\n",
       "      <td>0.978819</td>\n",
       "      <td>0.882557</td>\n",
       "      <td>0.881177</td>\n",
       "      <td>0.978758</td>\n",
       "      <td>0.978918</td>\n",
       "      <td>0.045394</td>\n",
       "      <td>0.046052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.946754</td>\n",
       "      <td>0.946304</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>{'logisticregression__lamduh': 0.01, 'standard...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.978945</td>\n",
       "      <td>0.978819</td>\n",
       "      <td>0.882557</td>\n",
       "      <td>0.881177</td>\n",
       "      <td>0.978758</td>\n",
       "      <td>0.978918</td>\n",
       "      <td>0.045394</td>\n",
       "      <td>0.046052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.946754</td>\n",
       "      <td>0.946304</td>\n",
       "      <td>0.1</td>\n",
       "      <td>True</td>\n",
       "      <td>{'logisticregression__lamduh': 0.1, 'standards...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.978945</td>\n",
       "      <td>0.978819</td>\n",
       "      <td>0.882557</td>\n",
       "      <td>0.881177</td>\n",
       "      <td>0.978758</td>\n",
       "      <td>0.978918</td>\n",
       "      <td>0.045394</td>\n",
       "      <td>0.046052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.946754</td>\n",
       "      <td>0.946304</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>{'logisticregression__lamduh': 0.1, 'standards...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.978945</td>\n",
       "      <td>0.978819</td>\n",
       "      <td>0.882557</td>\n",
       "      <td>0.881177</td>\n",
       "      <td>0.978758</td>\n",
       "      <td>0.978918</td>\n",
       "      <td>0.045394</td>\n",
       "      <td>0.046052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.946754</td>\n",
       "      <td>0.946304</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>{'logisticregression__lamduh': 1, 'standardsca...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.978945</td>\n",
       "      <td>0.978819</td>\n",
       "      <td>0.882557</td>\n",
       "      <td>0.881177</td>\n",
       "      <td>0.978758</td>\n",
       "      <td>0.978918</td>\n",
       "      <td>0.045394</td>\n",
       "      <td>0.046052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.946754</td>\n",
       "      <td>0.946304</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>{'logisticregression__lamduh': 1, 'standardsca...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.978945</td>\n",
       "      <td>0.978819</td>\n",
       "      <td>0.882557</td>\n",
       "      <td>0.881177</td>\n",
       "      <td>0.978758</td>\n",
       "      <td>0.978918</td>\n",
       "      <td>0.045394</td>\n",
       "      <td>0.046052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  mean_train_score param_logisticregression__lamduh  \\\n",
       "0         0.946754          0.946304                            0.001   \n",
       "1         0.946754          0.946304                            0.001   \n",
       "2         0.946754          0.946304                             0.01   \n",
       "3         0.946754          0.946304                             0.01   \n",
       "4         0.946754          0.946304                              0.1   \n",
       "5         0.946754          0.946304                              0.1   \n",
       "6         0.946754          0.946304                                1   \n",
       "7         0.946754          0.946304                                1   \n",
       "\n",
       "  param_standardscaler__with_std  \\\n",
       "0                           True   \n",
       "1                          False   \n",
       "2                           True   \n",
       "3                          False   \n",
       "4                           True   \n",
       "5                          False   \n",
       "6                           True   \n",
       "7                          False   \n",
       "\n",
       "                                              params  rank_test_score  \\\n",
       "0  {'logisticregression__lamduh': 0.001, 'standar...                1   \n",
       "1  {'logisticregression__lamduh': 0.001, 'standar...                1   \n",
       "2  {'logisticregression__lamduh': 0.01, 'standard...                1   \n",
       "3  {'logisticregression__lamduh': 0.01, 'standard...                1   \n",
       "4  {'logisticregression__lamduh': 0.1, 'standards...                1   \n",
       "5  {'logisticregression__lamduh': 0.1, 'standards...                1   \n",
       "6  {'logisticregression__lamduh': 1, 'standardsca...                1   \n",
       "7  {'logisticregression__lamduh': 1, 'standardsca...                1   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0           0.978945            0.978819           0.882557   \n",
       "1           0.978945            0.978819           0.882557   \n",
       "2           0.978945            0.978819           0.882557   \n",
       "3           0.978945            0.978819           0.882557   \n",
       "4           0.978945            0.978819           0.882557   \n",
       "5           0.978945            0.978819           0.882557   \n",
       "6           0.978945            0.978819           0.882557   \n",
       "7           0.978945            0.978819           0.882557   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  std_test_score  \\\n",
       "0            0.881177           0.978758            0.978918        0.045394   \n",
       "1            0.881177           0.978758            0.978918        0.045394   \n",
       "2            0.881177           0.978758            0.978918        0.045394   \n",
       "3            0.881177           0.978758            0.978918        0.045394   \n",
       "4            0.881177           0.978758            0.978918        0.045394   \n",
       "5            0.881177           0.978758            0.978918        0.045394   \n",
       "6            0.881177           0.978758            0.978918        0.045394   \n",
       "7            0.881177           0.978758            0.978918        0.045394   \n",
       "\n",
       "   std_train_score  \n",
       "0         0.046052  \n",
       "1         0.046052  \n",
       "2         0.046052  \n",
       "3         0.046052  \n",
       "4         0.046052  \n",
       "5         0.046052  \n",
       "6         0.046052  \n",
       "7         0.046052  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can do our usual checks on model fit for the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97888698720008394"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_train, y_train.values).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97886801935289736"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test, y_test.values).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully your reaction to everything here is somewhere between a nodding head and a yawn.\n",
    "If you're familiar with scikit-learn, everything here should look pretty routine.\n",
    "It's the same API you know and love, scaled out to larger datasets thanks to dask-glm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
