








<!DOCTYPE html>
<html lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Parallel Meta-estimators &mdash; dask-ml 1.3.1 documentation</title>
  

  
  
  
  

  
  <link rel="stylesheet" href="_static/css/style.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/explore.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/nbsphinx.css" type="text/css" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/javascript" src="_static/js/custom.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Incremental Learning" href="incremental.html" />
    <link rel="prev" title="Joblib" href="joblib.html" />
    <link rel="shortcut icon" href="_static/images/favicon.ico"/>
  
</head>

<body class="wy-body-for-nav">

  
    <nav id="explore-links">
        <a href="https://docs.dask.org/">
        <img class="caption" src="_static/images/dask-horizontal-white.svg"/>
        </a>

        <ul>
        <li>
            <a>Get Started</a>
            <ul>
            <li><a href="https://docs.dask.org/en/latest/install.html"> Install </a></li>
            <li><a href="https://examples.dask.org"> Examples </a></li>
            <li><a href="https://github.com/dask/dask-tutorial"> Tutorial </a></li>
            <li><a href="https://docs.dask.org/en/latest/why.html"> Why Dask? </a></li>
            <li><a href="https://stories.dask.org/en/latest"> Use Cases </a></li>
            <li><a href="https://www.youtube.com/watch?v=RA_2qdipVng&list=PLRtz5iA93T4PQvWuoMnIyEIz1fXiJ5Pri"> Talks </a></li>
            <li><a href="https://mybinder.org/v2/gh/dask/dask-examples/master?urlpath=lab"> Try Online </a></li>
            <li><a href="https://dask.org/slides"> Slides </a></li>
            </ul>
        </li>

        <li>
            <a href="">Algorithms</a>
            <ul>
            <li><a href="https://docs.dask.org/en/latest/array.html">Arrays</a></li>
            <li><a href="https://docs.dask.org/en/latest/dataframe.html">Dataframes</a></li>
            <li><a href="https://docs.dask.org/en/latest/bag.html">Bags</a></li>
            <li><a href="https://docs.dask.org/en/latest/delayed.html">Delayed (custom)</a></li>
            <li><a href="https://docs.dask.org/en/latest/futures.html">Futures (real-time)</a></li>
            <li><a href="http://ml.dask.org">Machine Learning</a></li>
            <li><a href="https://xarray.pydata.org/en/latest/">XArray</a></li>
            </ul>
        </li>

        <li>
            <a href="https://docs.dask.org/en/latest/setup.html">Setup</a>
            <ul>
            <li><a href="https://docs.dask.org/en/latest/setup/single-machine.html"> Local </a></li>
            <li><a href="https://docs.dask.org/en/latest/setup/cloud.html"> Cloud </a></li>
            <li><a href="https://docs.dask.org/en/latest/setup/hpc.html"> HPC </a></li>
            <li><a href="https://kubernetes.dask.org/en/latest/"> Kubernetes </a></li>
            <li><a href="https://yarn.dask.org/en/latest/"> Hadoop / Yarn </a></li>
            </ul>
        </li>

        <li>
            <a>Community</a>
            <ul>
            <li><a href="http://docs.dask.org/en/latest/support.html">Ask for Help</a></li>
            <li><a href="https://github.com/dask">Github</a></li>
            <li><a href="https://stackoverflow.com/questions/tagged/dask">Stack Overflow</a></li>
            <li><a href="https://twitter.com/dask_dev">Twitter</a></li>
            <li><a href="https://blog.dask.org/"> Developer Blog </a></li>
            </ul>
        </li>
        </ul>

    </nav>
  
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> dask-ml
          

          
          </a>

          
            
            
              <div class="version">
                1.3.1.dev14+gb812fe5
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
</ul>
<p class="caption"><span class="caption-text">Use</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="preprocessing.html">Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="cross_validation.html">Cross Validation</a></li>
<li class="toctree-l1"><a class="reference internal" href="hyper-parameter-search.html">Hyper Parameter Search</a></li>
<li class="toctree-l1"><a class="reference internal" href="compose.html">Pipelines and Composite Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="glm.html">Generalized Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="joblib.html">Joblib</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Parallel Meta-estimators</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#parallel-prediction-and-transformation">Parallel Prediction and Transformation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#comparison-to-other-estimators-in-dask-ml">Comparison to other Estimators in dask-ml</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="incremental.html">Incremental Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="clustering.html">Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="xgboost.html">XGBoost</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/api.html">API Reference</a></li>
</ul>
<p class="caption"><span class="caption-text">Develop</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="roadmap.html">Dask-ML Roadmap</a></li>
<li class="toctree-l1"><a class="reference internal" href="history.html">History</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">dask-ml</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Parallel Meta-estimators</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/meta-estimators.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="parallel-meta-estimators">
<span id="id1"></span><h1>Parallel Meta-estimators<a class="headerlink" href="#parallel-meta-estimators" title="Permalink to this headline">¶</a></h1>
<p>dask-ml provides some meta-estimators that parallelize and scaling out
certain tasks that may not be parallelized within scikit-learn itself.
For example, <a class="reference internal" href="modules/generated/dask_ml.wrappers.ParallelPostFit.html#dask_ml.wrappers.ParallelPostFit" title="dask_ml.wrappers.ParallelPostFit"><code class="xref py py-class docutils literal notranslate"><span class="pre">ParallelPostFit</span></code></a> will parallelize the
<code class="docutils literal notranslate"><span class="pre">predict</span></code>, <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> and <code class="docutils literal notranslate"><span class="pre">transform</span></code> methods, enabling them
to work on large (possibly larger-than memory) datasets.</p>
<div class="section" id="parallel-prediction-and-transformation">
<h2>Parallel Prediction and Transformation<a class="headerlink" href="#parallel-prediction-and-transformation" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="modules/generated/dask_ml.wrappers.ParallelPostFit.html#dask_ml.wrappers.ParallelPostFit" title="dask_ml.wrappers.ParallelPostFit"><code class="xref py py-class docutils literal notranslate"><span class="pre">wrappers.ParallelPostFit</span></code></a> is a meta-estimator for parallelizing
post-fit tasks like prediction and transformation. It can wrap any
scikit-learn estimator to provide parallel <code class="docutils literal notranslate"><span class="pre">predict</span></code>, <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code>, and
<code class="docutils literal notranslate"><span class="pre">transform</span></code> methods.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last"><code class="docutils literal notranslate"><span class="pre">ParallelPostFit</span></code> does <em>not</em> parallelize the training step. The underlying
estimator’s <code class="docutils literal notranslate"><span class="pre">.fit</span></code> method is called normally.</p>
</div>
<p>Since just the <code class="docutils literal notranslate"><span class="pre">predict</span></code>, <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code>, and <code class="docutils literal notranslate"><span class="pre">transform</span></code> methods are
wrapped, <a class="reference internal" href="modules/generated/dask_ml.wrappers.ParallelPostFit.html#dask_ml.wrappers.ParallelPostFit" title="dask_ml.wrappers.ParallelPostFit"><code class="xref py py-class docutils literal notranslate"><span class="pre">wrappers.ParallelPostFit</span></code></a> is most useful in situations where
your training dataset is relatively small (fits in a single machine’s memory),
and prediction or transformation must be done on a much larger dataset (perhaps
larger than a single machine’s memory).</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>

<span class="gp">In [2]: </span><span class="kn">import</span> <span class="nn">sklearn.datasets</span>

<span class="gp">In [3]: </span><span class="kn">import</span> <span class="nn">dask_ml.datasets</span>

<span class="gp">In [4]: </span><span class="kn">from</span> <span class="nn">dask_ml.wrappers</span> <span class="kn">import</span> <span class="n">ParallelPostFit</span>
</pre></div>
</div>
<p>In this example, we’ll make a small 1,000 sample training dataset</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [5]: </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="gp">   ...: </span>                                            <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">   ...: </span>
</pre></div>
</div>
<p>Training is identical to just calling <code class="docutils literal notranslate"><span class="pre">estimator.fit(X,</span> <span class="pre">y)</span></code>. Aside from
copying over learned attributes, that’s all that <code class="docutils literal notranslate"><span class="pre">ParallelPostFit</span></code> does.</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [6]: </span><span class="n">clf</span> <span class="o">=</span> <span class="n">ParallelPostFit</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">GradientBoostingClassifier</span><span class="p">())</span>

<span class="gp">In [7]: </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gh">Out[7]: </span><span class="go"></span>
<span class="go">ParallelPostFit(estimator=GradientBoostingClassifier(ccp_alpha=0.0,</span>
<span class="go">                                                     criterion=&#39;friedman_mse&#39;,</span>
<span class="go">                                                     init=None,</span>
<span class="go">                                                     learning_rate=0.1,</span>
<span class="go">                                                     loss=&#39;deviance&#39;,</span>
<span class="go">                                                     max_depth=3,</span>
<span class="go">                                                     max_features=None,</span>
<span class="go">                                                     max_leaf_nodes=None,</span>
<span class="go">                                                     min_impurity_decrease=0.0,</span>
<span class="go">                                                     min_impurity_split=None,</span>
<span class="go">                                                     min_samples_leaf=1,</span>
<span class="go">                                                     min_samples_split=2,</span>
<span class="go">                                                     min_weight_fraction_leaf=0.0,</span>
<span class="go">                                                     n_estimators=100,</span>
<span class="go">                                                     n_iter_no_change=None,</span>
<span class="go">                                                     presort=&#39;deprecated&#39;,</span>
<span class="go">                                                     random_state=None,</span>
<span class="go">                                                     subsample=1.0, tol=0.0001,</span>
<span class="go">                                                     validation_fraction=0.1,</span>
<span class="go">                                                     verbose=0,</span>
<span class="go">                                                     warm_start=False),</span>
<span class="go">                scoring=None)</span>
</pre></div>
</div>
<p>This class is useful for predicting for or transforming large datasets.
We’ll make a larger dask array <code class="docutils literal notranslate"><span class="pre">X_big</span></code> with 10,000 samples per block.</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [8]: </span><span class="n">X_big</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">dask_ml</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span>
<span class="gp">   ...: </span>                                                <span class="n">chunks</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
<span class="gp">   ...: </span>                                                <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">   ...: </span>

<span class="gp">In [9]: </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_big</span><span class="p">)</span>
<span class="gh">Out[9]: </span><span class="go">dask.array&lt;_predict, shape=(100000,), dtype=int64, chunksize=(10000,), chunktype=numpy.ndarray&gt;</span>
</pre></div>
</div>
<p>This returned a <code class="docutils literal notranslate"><span class="pre">dask.array</span></code>. Like any dask array, the actual <code class="docutils literal notranslate"><span class="pre">compute</span></code> will
cause the scheduler to compute tasks in parallel. If you’ve connected to a
<code class="docutils literal notranslate"><span class="pre">dask.distributed.Client</span></code>, the computation will be parallelized across your
cluster of machines.</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [10]: </span><span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_big</span><span class="p">)</span><span class="o">.</span><span class="n">compute</span><span class="p">()[:</span><span class="mi">10</span><span class="p">]</span>
<span class="gh">Out[10]: </span><span class="go"></span>
<span class="go">array([[0.24519991, 0.75480009],</span>
<span class="go">       [0.00464304, 0.99535696],</span>
<span class="go">       [0.00902734, 0.99097266],</span>
<span class="go">       [0.01299259, 0.98700741],</span>
<span class="go">       [0.90754654, 0.09245346],</span>
<span class="go">       [0.755881  , 0.244119  ],</span>
<span class="go">       [0.94124115, 0.05875885],</span>
<span class="go">       [0.0203811 , 0.9796189 ],</span>
<span class="go">       [0.021179  , 0.978821  ],</span>
<span class="go">       [0.95218171, 0.04781829]])</span>
</pre></div>
</div>
<p>See <a class="reference external" href="http://dask-ml-benchmarks.readthedocs.io/en/latest/auto_examples/plot_parallel_post_fit_scaling.html">parallelizing prediction</a> for an example of how this
scales for a support vector classifier.</p>
</div>
<div class="section" id="comparison-to-other-estimators-in-dask-ml">
<h2>Comparison to other Estimators in dask-ml<a class="headerlink" href="#comparison-to-other-estimators-in-dask-ml" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">dask-ml</span></code> re-implements some estimators from scikit-learn, for example
<a class="reference internal" href="modules/generated/dask_ml.cluster.KMeans.html#dask_ml.cluster.KMeans" title="dask_ml.cluster.KMeans"><code class="xref py py-class docutils literal notranslate"><span class="pre">dask_ml.cluster.KMeans</span></code></a>, or <a class="reference internal" href="modules/generated/dask_ml.preprocessing.QuantileTransformer.html#dask_ml.preprocessing.QuantileTransformer" title="dask_ml.preprocessing.QuantileTransformer"><code class="xref py py-class docutils literal notranslate"><span class="pre">dask_ml.preprocessing.QuantileTransformer</span></code></a>. This raises
the question, should I use the reimplemented dask-ml versions, or should I wrap
scikit-learn version in a meta-estimator? It varies from estimator to estimator,
and depends on your tolerance for approximate solutions and the size of your
training data. In general, if your training data is small, you should be fine
wrapping the scikit-learn version with a <code class="docutils literal notranslate"><span class="pre">dask-ml</span></code> meta-estimator.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="incremental.html" class="btn btn-neutral float-right" title="Incremental Learning" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="joblib.html" class="btn btn-neutral float-left" title="Joblib" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Dask developers

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>